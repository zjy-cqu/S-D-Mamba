Args in experiment:
Namespace(is_training=1, model_id='ETTh1_96_96', model='iMamba', data='ETTh1', root_path='./dataset/ETT-small/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, enc_in=7, dec_in=7, c_out=7, d_model=256, n_heads=8, e_layers=2, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=7e-05, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', exp_name='MTSF', channel_independence=False, inverse=False, class_strategy='projection', target_root_path='./data/electricity/', target_data_path='electricity.csv', efficient_training=False, use_norm=True, partial_start_index=0, dt_rank=32, patch_num=32, d_state=16, d_conv=4, expand=2, dt_min=0.001, dt_init='random', dt_max=0.1, dt_scale=1.0, dt_init_floor=0.0001, bias=True, conv_bias=True, pscan=False, avg=False, max=False, reduction=2, gddmlp=False, channel_mixup=False, sigma=1.0)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_96_96_iMamba_ETTh1_M_ft96_sl48_ll96_pl256_dm8_nh2_el1_dl256_df1_fctimeF_ebTrue_dtExp_projection_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8449
val 2785
test 2785
	iters: 100, epoch: 1 | loss: 0.4511436
	speed: 0.0167s/iter; left time: 42.5226s
	iters: 200, epoch: 1 | loss: 0.3894088
	speed: 0.0120s/iter; left time: 29.2992s
Epoch: 1 cost time: 3.686952829360962
Epoch: 1, Steps: 264 | Train Loss: 0.4662441 Vali Loss: 0.7272595 Test Loss: 0.4132758
Validation loss decreased (inf --> 0.727260).  Saving model ...
Updating learning rate to 7e-05
	iters: 100, epoch: 2 | loss: 0.4568604
	speed: 0.1049s/iter; left time: 238.8229s
	iters: 200, epoch: 2 | loss: 0.4916721
	speed: 0.0124s/iter; left time: 27.0509s
Epoch: 2 cost time: 3.3492047786712646
Epoch: 2, Steps: 264 | Train Loss: 0.3815888 Vali Loss: 0.7012433 Test Loss: 0.3953860
Validation loss decreased (0.727260 --> 0.701243).  Saving model ...
Updating learning rate to 3.5e-05
	iters: 100, epoch: 3 | loss: 0.3241430
	speed: 0.1052s/iter; left time: 211.7490s
	iters: 200, epoch: 3 | loss: 0.3500150
	speed: 0.0102s/iter; left time: 19.4266s
Epoch: 3 cost time: 2.9310781955718994
Epoch: 3, Steps: 264 | Train Loss: 0.3657567 Vali Loss: 0.6932347 Test Loss: 0.3914529
Validation loss decreased (0.701243 --> 0.693235).  Saving model ...
Updating learning rate to 1.75e-05
	iters: 100, epoch: 4 | loss: 0.3571495
	speed: 0.1105s/iter; left time: 193.1958s
	iters: 200, epoch: 4 | loss: 0.3811323
	speed: 0.0135s/iter; left time: 22.2248s
Epoch: 4 cost time: 3.5877416133880615
Epoch: 4, Steps: 264 | Train Loss: 0.3593910 Vali Loss: 0.6921139 Test Loss: 0.3906064
Validation loss decreased (0.693235 --> 0.692114).  Saving model ...
Updating learning rate to 8.75e-06
	iters: 100, epoch: 5 | loss: 0.3727629
	speed: 0.0856s/iter; left time: 127.1235s
	iters: 200, epoch: 5 | loss: 0.3359048
	speed: 0.0117s/iter; left time: 16.2485s
Epoch: 5 cost time: 3.2709360122680664
